import os
os.environ["CUDA_VISIBLE_DEVICES"]="0"

import numpy as np
import tensorflow as tf
from sklearn.utils.class_weight import compute_class_weight
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from display_utils import DynamicConsoleTable
import math
import time
import os.path

import data

abs_path = os.path.abspath(os.path.dirname(__file__))

def normalize_kernel(kernel, subtract_mean=False):
    if subtract_mean:
        kernel = np.array(kernel, np.float32) - np.mean(kernel)
    return np.array(kernel, np.float32) / np.sum(np.abs(kernel))
def ricker_function(t, sigma):
    return 2./(np.sqrt(3*sigma)*np.pi**0.25)*(1.-(float(t)/sigma)**2)*np.exp(-(float(t)**2)/(2*sigma**2))
def ricker_wavelet(n, sigma):
    return np.array(map(lambda x: ricker_function(x, sigma), range(-n//2, n//2+1)))
    
def transform_data(sequence_groups, sample_rate=250):
    #### Apply DC offset and drift correction
    drift_low_freq = 0.5 #0.5
    sequence_groups = data.transform.subtract_initial(sequence_groups)
    sequence_groups = data.transform.highpass_filter(sequence_groups, drift_low_freq, sample_rate)
    sequence_groups = data.transform.subtract_mean(sequence_groups)

    #### Apply notch filters at multiples of notch_freq
    notch_freq = 60
    num_times = 3 #pretty much just the filter order
    freqs = map(int, map(round, np.arange(1, sample_rate/(2. * notch_freq)) * notch_freq))
    for _ in range(num_times):
        for f in reversed(freqs):
            sequence_groups = data.transform.notch_filter(sequence_groups, f, sample_rate)

    #### Apply standard deviation normalization
    #sequence_groups = data.transform.normalize_std(sequence_groups)

    #### Apply ricker wavelet subtraction
    ricker_width = 35 * sample_rate // 250
    ricker_sigma = 4.0 * sample_rate / 250
    ricker_kernel = normalize_kernel(ricker_wavelet(ricker_width, ricker_sigma))
    ricker_convolved = data.transform.correlate(sequence_groups, ricker_kernel)
    ricker_subtraction_multiplier = 2.0
    sequence_groups = sequence_groups - ricker_subtraction_multiplier * ricker_convolved

    #### Apply sine wavelet kernel
    #period = int(sample_rate)
    #sin_kernel = normalize_kernel(np.sin(np.arange(period)/float(period) * 1*np.pi), subtract_mean=True)
    #sequence_groups = data.transform.correlate(sequence_groups, sin_kernel)

    low_freq = 0.5 #0.5
    high_freq = 8 #8
    order = 1

    #### Apply soft bandpassing
    sequence_groups = data.transform.bandpass_filter(sequence_groups, low_freq, high_freq, sample_rate, order=order)
    
    #### Apply hard bandpassing
    #sequence_groups = data.transform.fft(sequence_groups)
    #sequence_groups = data.transform.fft_frequency_cutoff(sequence_groups, low_freq, high_freq, sample_rate)
    #sequence_groups = np.real(data.transform.ifft(sequence_groups))
#    
    return sequence_groups

#length = 3000 #3000 #600 #2800
    
    
channels = range(0, 8)
surrounding = 250 #250

labels = list(range(100, 400))

train_1 = data.process_scrambled(labels, ['phoneme400_train1.txt'], channels=channels, sample_rate=250,
                                 surrounding=surrounding, exclude=set([]), num_classes=400)

labels = list(range(0, 100))

test_1 = data.process_scrambled(labels, ['phoneme400_test1.txt'], channels=channels, sample_rate=250,
                                 surrounding=surrounding, exclude=set([]), num_classes=400)

labels = list(range(100, 400))

train_2 = data.process_scrambled(labels, ['phoneme400_train2.txt'], channels=channels, sample_rate=250,
                                 surrounding=surrounding, exclude=set([]), num_classes=400)

labels = list(range(0, 100))

test_2 = data.process_scrambled(labels, ['phoneme400_test2.txt'], channels=channels, sample_rate=250,
                                 surrounding=surrounding, exclude=set([]), num_classes=400)

training_sequence_groups = data.combine([train_1, train_2])
validation_sequence_groups = data.combine([test_1, test_2])

print len(training_sequence_groups)
print map(len, training_sequence_groups)

lens = map(len, data.get_inputs(training_sequence_groups)[0])
print min(lens), np.mean(lens), max(lens)

print len(validation_sequence_groups)
print map(len, validation_sequence_groups)

lens = map(len, data.get_inputs(validation_sequence_groups)[0])
print min(lens), np.mean(lens), max(lens)


# Format into sequences and labels
train_sequences, train_labels = data.get_inputs(training_sequence_groups)
val_sequences, val_labels = data.get_inputs(validation_sequence_groups)


train_sequences = transform_data(train_sequences)
val_sequences = transform_data(val_sequences)


# Calculate sample weights
#class_weights = compute_class_weight('balanced', np.unique(train_labels), train_labels)
#train_weights = class_weights[list(train_labels)]
train_weights = np.ones(len(train_labels))

#train_labels = tf.keras.utils.to_categorical(train_labels)
#val_labels = tf.keras.utils.to_categorical(val_labels)

words = np.array(['aa', 'iy', 'ch', 'ae', 'eh', 'ah', 'ao', 'ih', 'ey', 'aw', 'ay', 'zh', 'er', 'ng', 'sh', 'th', 'uh', 'w', 'dh', 'y', 'hh', 'jh', 'b', 'd', 'g', 'f', 'k', 'm', 'l', 'n', 'p', 's', 'r', 't', 'oy', 'v', 'ow', 'z', 'uw'])

label_map = [[18, 5, 26, 9, 22, 34, 26, 6, 28, 23, 18, 7, 31, 22, 32, 1, 23, 5, 35, 26, 3, 33, 5, 28, 27, 3, 24, 30, 10, 37], [18, 5, 4, 29, 5, 27, 1, 23, 7, 23, 29, 0, 33, 35, 7, 32], [20, 7, 37, 22, 0, 23, 1, 30, 7, 2, 33, 22, 3, 26, 5, 29, 23, 25, 6, 32, 15, 0, 29, 18, 5, 22, 4, 29, 2], [22, 5, 33, 7, 33, 23, 7, 23, 30, 32, 7, 29, 33, 24, 16, 23, 35, 12, 31, 5, 29, 23, 24, 16, 23, 25, 7, 26, 14, 5, 29], [7, 33, 7, 37, 5, 20, 3, 30, 1, 22, 5, 37, 7, 13, 26, 32, 9, 23], [18, 5, 27, 3, 31, 33, 12, 37, 20, 3, 29, 23, 20, 3, 37, 28, 6, 31, 33, 29, 5, 29, 5, 35, 7, 33, 31, 26, 32, 3, 25, 33], [29, 6, 32, 17, 12, 20, 7, 37, 27, 3, 29, 12, 37, 22, 0, 32, 22, 3, 32, 7, 26], [20, 1, 17, 0, 37, 29, 1, 28, 7, 13, 33, 38, 33, 10, 20, 7, 37, 14, 38, 28, 8, 31, 5, 37], [29, 9, 20, 4, 28, 2, 36, 26, 25, 6, 32, 14, 16, 32], [31, 30, 28, 4, 29, 23, 12, 22, 10, 31, 6, 32, 31, 12, 1, 7, 33, 31, 5, 20, 6, 32, 12], [15, 1, 0, 26, 32, 5, 31, 1, 32, 1, 26, 5, 29, 31, 7, 23, 12, 23], [17, 7, 18, 18, 7, 31, 29, 36, 28, 34, 5, 28, 31, 7, 33, 5, 37, 5, 29, 26, 3, 29, 26, 17, 6, 32, 5, 28], [7, 33, 27, 36, 33, 5, 35, 8, 33, 31, 20, 7, 37, 22, 7, 20, 8, 35, 19, 12], [22, 5, 33, 17, 1, 26, 3, 29, 0, 33, 31, 33, 0, 32, 33, 6, 25, 17, 7, 18, 5, 26, 28, 1, 29, 31, 28, 8, 33], [30, 28, 8, 31, 17, 12, 26, 0, 29, 5, 25, 28, 3, 33, 31, 12, 25, 5, 31, 5, 29, 23, 31, 27, 38, 18, 9, 33], [17, 10, 14, 16, 23, 18, 7, 31, 31, 5, 23, 5, 29, 28, 1, 5, 31, 8, 28, 20, 12], [17, 7, 18, 5, 17, 12, 28, 7, 13, 21, 5, 27, 30, 7, 33, 26, 16, 23, 24, 4, 33, 7, 29, 33, 38, 24, 7, 32], [28, 4, 33, 31, 33, 8, 26, 4, 27, 20, 36, 27], [20, 1, 14, 32, 5, 24, 23, 26, 3, 11, 5, 17, 5, 28, 1], [18, 4, 29, 6, 25, 5, 24, 4, 29, 32, 5, 14, 7, 13, 33, 38, 26, 1, 30, 5, 30], [19, 38, 14, 3, 28, 31, 1, 20, 12, 3, 33, 23, 7, 29, 12], [3, 37, 14, 1, 23, 32, 36, 35, 14, 1, 15, 6, 33, 5, 22, 9, 33, 20, 12, 30, 28, 3, 29], [21, 36, 26, 7, 13, 31, 33, 0, 30, 33, 5, 29, 23, 4, 35, 32, 1, 17, 5, 29, 24, 3, 18, 12, 23, 12, 9, 29, 23], [20, 1, 12, 29, 5, 31, 33, 28, 1, 12, 21, 23, 5, 31, 1, 31, 25, 10, 12], [18, 8, 17, 12, 6, 28, 24, 16, 23, 27, 4, 29], [21, 5, 31, 33, 28, 4, 33, 27, 1, 23, 10, 7, 29, 30, 1, 31], [18, 8, 20, 3, 23, 26, 5, 27, 29, 0, 33, 33, 38, 3, 23, 27, 10, 32, 22, 5, 33, 33, 38, 5, 22, 37, 12, 35], [18, 5, 28, 1, 24, 37, 0, 32, 25, 16, 28, 5, 35, 24, 10, 37, 28, 10, 26, 18, 3, 33], [31, 8, 20, 1, 7, 37, 5, 20, 6, 32, 31, 15, 1, 25, 32, 5, 29, 37, 3, 29, 36, 28, 23, 3, 23, 5, 21], [33, 8, 26, 19, 6, 32, 26, 0, 22, 28, 12, 37, 14, 0, 30, 31, 5, 27, 17, 4, 32, 4, 28, 31], [29, 4, 35, 12, 18, 5, 28, 4, 31, 7, 33, 31, 33, 32, 38], [20, 1, 22, 32, 6, 33, 20, 12, 14, 6, 28, 37], [5, 33, 0, 27, 7, 26, 4, 29, 12, 21, 1, 28, 4, 35, 5, 28, 37], [25, 10, 29, 23, 8, 3, 25, 33, 12, 33, 5, 27, 0, 32, 36, 14, 1, 3, 23, 5, 23], [28, 16, 26, 31, 5, 27, 17, 4, 32, 4, 28, 31], [27, 8, 26, 7, 33, 26, 5, 27, 6, 25, 6, 28, 32, 10, 33], [26, 16, 23, 7, 33, 23, 38, 5, 18, 12, 17, 10, 37], [31, 36, 5, 35, 26, 6, 32, 31, 20, 1, 31, 33, 8, 23, 30, 16, 33], [18, 5, 24, 5, 33, 12, 5, 28, 28, 3, 13, 24, 17, 5, 21, 25, 32, 5, 27, 18, 5, 24, 4, 33, 36, 31, 33, 0, 30, 33], [20, 1, 27, 8, 24, 4, 33, 5, 33, 3, 26, 31, 32, 7, 25, 5, 29, 23], [25, 19, 38, 27, 7, 13, 20, 4, 28, 30, 28, 5, 31, 20, 1, 17, 0, 2, 33, 18, 4, 27, 30, 3, 31, 20, 7, 27], [20, 12, 26, 0, 25, 31, 9, 29, 23, 5, 23, 28, 10, 26, 26, 28, 6, 15, 32, 7, 30, 7, 13], [18, 7, 31, 7, 37, 9, 12, 31, 7, 13, 24, 19, 5, 28, 12, 24, 36, 28], [20, 38, 33, 36, 28, 23, 19, 38, 33, 38, 23, 38, 7, 33], [31, 5, 30, 36, 37, 19, 38, 33, 4, 28, 27, 1, 18, 5, 32, 1, 28, 32, 1, 37, 5, 29, 20, 1, 23, 32, 6, 28, 23], [17, 5, 33, 31, 19, 6, 32, 29, 8, 27, 4, 29, 1, 17, 8], [17, 1, 20, 3, 35, 22, 4, 33, 12, 32, 38, 27, 37, 35, 8, 26, 5, 29, 33, 29, 9, 20, 1, 22, 3, 22, 5, 28, 23], [5, 15, 12, 23, 35, 0, 28, 19, 38, 27, 32, 7, 27, 8, 29, 37, 33, 38, 22, 1, 30, 5, 22, 28, 7, 14, 33], [0, 29, 18, 5, 22, 4, 32, 25, 28, 6, 32, 22, 6, 32, 23, 37, 5, 27, 3, 29, 28, 8, 25, 8, 31, 23, 9, 29], [5, 22, 8, 5, 29, 4, 33, 20, 5, 13, 7, 29, 5, 22, 4, 28, 33, 31, 26, 3, 22, 12, 23], [29, 0, 33, 17, 4, 29, 10, 17, 0, 37, 5, 26, 7, 23], [27, 8, 10, 24, 4, 33, 31, 5, 27, 27, 6, 32, 26, 16, 26, 1, 37, 29, 6, 32, 5], [19, 38, 24, 0, 29, 5, 24, 7, 35, 27, 1, 5, 23, 32, 7, 13, 26, 25, 4, 28, 5], [20, 5, 29, 1, 20, 1, 17, 7, 31, 30, 12, 23], [18, 5, 24, 12, 7, 28, 5, 22, 7, 35, 17, 3, 26, 32, 7, 27, 8, 29, 23, 31, 10, 28, 5, 29, 33], [18, 4, 29, 20, 1, 15, 6, 33, 27, 1, 27, 6, 32, 30, 12, 35, 12, 31, 18, 3, 29, 4, 35, 12], [2, 8, 29, 21, 7, 29, 35, 0, 28, 35, 37, 18, 5, 23, 7, 31, 30, 28, 8, 31, 27, 5, 29, 33, 5, 35, 25, 6, 32, 27], [25, 12, 24, 4, 33, 17, 1, 4, 35, 12, 29, 38, 17, 5, 33], [25, 7, 23, 5, 28, 37, 31, 26, 32, 1, 2, 33, 5, 30, 1, 3, 29, 36, 33, 7, 13, 26, 5, 28, 23], [29, 36, 18, 8, 26, 16, 23, 26, 7, 28, 20, 7, 27, 21, 5, 31, 33, 3, 37, 1, 37, 1, 32, 10, 33, 29, 9], [7, 33, 20, 3, 23, 24, 6, 29, 28, 10, 26, 26, 28, 0, 26, 17, 12, 26], [29, 8, 27, 23, 3, 25, 33, 12, 18, 5, 22, 3, 28, 8], [18, 4, 29, 20, 1, 20, 12, 23, 18, 5, 9, 33, 12, 23, 6, 32, 26, 28, 36, 37, 7, 13], [18, 8, 20, 3, 23, 2, 36, 37, 5, 29, 18, 7, 31, 29, 10, 33, 30, 12, 30, 5, 31, 28, 1], [18, 5, 31, 12, 21, 5, 29, 26, 3, 32, 1, 23, 5, 26, 8, 21, 5, 35, 28, 10, 35, 30, 7, 21, 5, 29, 37], [22, 5, 33, 29, 9, 14, 1, 31, 3, 13, 26, 28, 36, 12, 33, 38, 18, 5, 32, 0, 26], [18, 8, 31, 1, 27, 23, 0, 32, 29, 23, 30, 32, 9, 23, 5, 35, 7, 33], [18, 4, 32, 17, 12, 29, 36, 33, 32, 3, 26, 31, 5, 35, 1, 18, 12, 20, 16, 25, 31, 6, 32, 22, 38, 33, 31], [20, 12, 28, 7, 30, 31, 27, 34, 31, 33, 5, 29, 23, 30, 0, 32, 33, 5, 23, 31, 30, 36, 26, 20, 7, 37, 29, 8, 27], [20, 36, 27, 27, 8, 23, 31, 9, 12, 26, 32, 9, 33, 7, 37, 31, 12, 35, 23, 17, 5, 29, 31, 5, 17, 1, 26], [20, 1, 2, 5, 26, 5, 28, 23, 18, 5, 27, 4, 27, 12, 1, 35, 7, 35, 5, 23], [19, 4, 31, 19, 4, 31, 17, 4, 28, 28, 4, 33, 19, 38, 29, 36], [33, 32, 7, 27, 4, 26, 31, 4, 31, 26, 28, 8, 5, 17, 8, 25, 32, 5, 27, 9, 33, 12, 4, 21, 5, 37], [31, 12, 35, 7, 29, 25, 32, 3, 13, 26, 25, 12, 33, 12, 22, 5, 29, 37, 6, 32, 3, 37, 5, 27, 1, 33, 23, 7, 14], [18, 8, 17, 12, 6, 28, 25, 8, 23, 5, 23], [18, 4, 29, 3, 25, 33, 12, 5, 17, 10, 28, 14, 1, 17, 4, 29, 33, 33, 38, 20, 12, 27, 7, 32, 12], [17, 1, 15, 7, 13, 26, 23, 7, 25, 32, 5, 29, 33, 28, 1], [29, 9, 20, 1, 32, 7, 37, 33, 32, 38, 28, 1, 5, 27, 0, 32, 35, 5, 28], [20, 7, 37, 10, 37, 22, 12, 29, 23, 25, 4, 35, 12, 7, 14, 28, 1], [18, 4, 32, 17, 0, 37, 33, 10, 25, 34, 23, 5, 29, 23, 27, 5, 28, 4, 32, 1, 5], [27, 7, 28, 26, 5, 30, 7, 32, 37, 33, 17, 10, 31, 5, 23, 8], [20, 4, 28, 14, 38, 33, 5, 30, 18, 5, 33, 9, 29], [20, 1, 32, 18, 8, 23, 7, 23, 29, 0, 33, 29, 1, 23, 33, 38, 22, 1, 7, 29, 24, 4, 33, 36, 37], [3, 37, 17, 1, 8, 33, 17, 1, 33, 6, 26, 33], [18, 8, 23, 7, 31, 30, 10, 37, 23, 25, 6, 32, 5, 29, 12, 37], [20, 1, 27, 8, 33, 32, 10, 33, 38, 25, 36, 29, 5, 31], [7, 33, 17, 0, 37, 31, 7, 26, 19, 16, 32, 23, 22, 10, 3, 29, 36, 35, 12, 31, 10, 37, 23, 30, 3, 23, 28, 0, 26], [18, 5, 10, 7, 37, 6, 28, 7, 29, 17, 12, 23, 6, 32, 9, 33, 17, 12, 23], [0, 32, 19, 38, 5, 29, 23, 12, 32, 10, 33, 7, 13, 7, 26, 31, 30, 4, 29, 31, 7, 35, 33, 1, 27, 33, 32, 7, 30, 31], [27, 4, 28, 5, 29, 26, 0, 28, 1, 5, 22, 31, 4, 14, 5, 29], [18, 7, 31, 27, 1, 29, 37, 1, 18, 12, 26, 0, 32, 37, 6, 32, 33, 32, 5, 26, 31], [18, 4, 29, 18, 5, 26, 6, 32, 1, 0, 24, 32, 5, 25, 12, 27, 5, 31, 33, 0, 32, 22, 5, 33, 32, 8, 33], [27, 8, 10, 24, 36, 5, 29, 23, 24, 4, 33, 26, 5, 31, 3, 29, 23, 32, 5, 27, 0, 27, 5], [17, 7, 18, 20, 7, 37, 26, 28, 5, 22, 25, 16, 33, 20, 1, 27, 10, 33, 17, 4, 28, 22, 1, 24, 32, 8, 33, 25, 5, 28], [25, 7, 28, 31, 27, 6, 28, 20, 36, 28, 7, 29, 22, 36, 28, 17, 7, 18, 26, 28, 8], [7, 29, 5, 17, 1, 26, 18, 5, 20, 10, 5, 31, 7, 29, 15, 31, 17, 16, 23, 31, 30, 10, 26, 9, 33], [17, 12, 19, 38, 0, 29, 23, 38, 33, 1, 20, 1, 32, 33, 38, 17, 1, 26, 31, 5, 24, 36], [20, 9, 19, 0, 24, 0, 29, 5, 26, 1, 30, 4, 27, 23, 9, 29, 0, 29, 18, 5, 25, 0, 32, 27], [31, 33, 8, 32, 10, 33, 20, 1, 32, 17, 4, 32, 19, 38, 0, 32, 26, 7, 23, 20, 1, 26, 6, 28, 23], [27, 8, 22, 1, 18, 8, 6, 28, 32, 4, 23, 1, 24, 0, 33, 18, 4, 27], [31, 36, 17, 5, 33, 31, 18, 7, 31, 6, 28, 5, 22, 9, 33], [25, 38, 28, 37, 20, 1, 22, 8, 23, 17, 5, 33, 23, 38, 19, 38, 15, 7, 13, 26, 19, 38, 0, 32, 23, 38, 7, 13], [27, 8, 22, 1, 33, 17, 4, 29, 33, 1, 15, 12, 23, 1, 25, 7, 25, 33, 1], [17, 12, 24, 36, 7, 13, 31, 5, 27, 30, 28, 8, 31], [5, 15, 12, 23, 14, 0, 33, 23, 9, 31, 33, 18, 5, 28, 10, 33], [20, 7, 37, 31, 17, 1, 33, 17, 7, 31, 30, 12, 26, 8, 27, 3, 25, 33, 12, 24, 32, 8, 33, 4, 25, 12, 33], [10, 31, 6, 19, 6, 32, 20, 6, 32, 31, 9, 33, 31, 10, 23], [23, 38, 19, 38, 6, 28, 17, 8, 37, 29, 3, 35, 5, 24, 8, 33, 28, 10, 26, 18, 7, 31], [20, 1, 24, 8, 37, 23, 5, 17, 8, 25, 32, 5, 27, 5, 31, 3, 37, 17, 1, 5, 30, 32, 36, 2, 33], [28, 4, 33, 31, 20, 36, 30, 17, 1, 26, 5, 27, 33, 38, 5, 31, 8, 25, 12, 30, 28, 8, 31], [20, 9, 23, 38, 18, 8, 33, 12, 29, 9, 33, 28, 8, 33, 12], [20, 7, 37, 33, 4, 26, 29, 1, 26, 7, 37, 21, 4, 29, 19, 5, 17, 5, 29, 28, 1, 27, 3, 31, 33, 12, 25, 5, 28], [18, 5, 27, 3, 29, 37, 10, 28, 7, 23, 37, 25, 28, 5, 33, 12, 23], [18, 8, 6, 25, 5, 29, 15, 32, 38, 33, 7, 23, 22, 7, 33, 31, 36, 35, 12, 22, 6, 32, 23], [17, 12, 19, 38, 7, 29, 28, 5, 35, 17, 7, 18, 18, 3, 33, 24, 12, 28], [20, 1, 31, 26, 9, 28, 23, 3, 33, 20, 12, 25, 28, 9, 12, 37], [15, 32, 4, 23, 27, 3, 2, 7, 13, 19, 0, 32, 29, 7, 29, 33, 3, 30, 5, 31, 33, 32, 1, 29, 1, 23, 5, 28], [20, 1, 20, 38, 23, 5, 37, 29, 0, 33, 28, 5, 35, 5, 22, 10, 23, 37, 7, 29, 23, 4, 15], [20, 1, 25, 4, 28, 33, 5, 24, 16, 23, 23, 1, 28, 28, 4, 31, 14, 8, 26, 1], [23, 32, 5, 24, 37, 26, 4, 27, 5, 26, 5, 28, 29, 8, 27, 30, 32, 36, 26, 8, 29, 30, 4, 29, 5, 31, 7, 28, 5, 29], [30, 3, 29, 37, 1, 37, 0, 32, 24, 28, 5, 33, 5, 29, 37], [28, 8, 33, 12, 19, 38, 14, 3, 28, 29, 36, 7, 33, 22, 4, 33, 12], [29, 36, 30, 32, 10, 31, 7, 37, 33, 38, 20, 10, 17, 4, 29, 33, 32, 38, 28, 5, 35, 7, 37, 3, 33, 31, 33, 8, 26], [33, 6, 31, 5, 23, 10, 5, 29, 33, 7, 28, 3, 29, 8, 31, 5, 30, 7, 32, 37], [5, 26, 32, 38, 23, 28, 3, 23, 12, 32, 3, 29, 23, 9, 29, 33, 38, 5, 17, 16, 23, 5, 29, 25, 28, 6, 32], [27, 1, 20, 1, 31, 4, 23, 27, 4, 32, 5, 28, 1], [29, 9, 25, 12, 24, 4, 33, 6, 28, 18, 7, 31, 5, 18, 12], [20, 38, 17, 16, 23, 33, 8, 26, 36, 35, 12], [18, 5, 31, 0, 25, 33, 31, 29, 36, 17, 0, 37, 23, 5, 31, 1, 33, 25, 5, 28, 5, 29, 23, 12, 25, 16, 33], [18, 1, 37, 6, 28, 17, 8, 37, 26, 5, 29, 33, 8, 29, 27, 5, 33, 3, 28, 7, 26, 7, 29, 26, 28, 38, 11, 5, 29, 37], [7, 33, 28, 16, 26, 31, 28, 10, 26, 17, 1, 23, 7, 23, 17, 4, 29, 17, 1, 27, 8, 23, 22, 28, 3, 31, 33, 23, 9, 29], [33, 38, 27, 4, 29, 1, 20, 3, 35, 22, 0, 24, 23, 23, 9, 29, 7, 29, 22, 7, 26, 12, 7, 13], [31, 5, 2, 5, 26, 0, 27, 5, 29, 23, 5, 14, 16, 32, 7, 13, 30, 1, 31, 26, 3, 29, 22, 1, 19, 16, 32, 37], [18, 8, 1, 35, 7, 29, 30, 8, 27, 1, 31, 7, 26, 31, 23, 0, 28, 12, 37, 5, 27, 5, 29, 15], [7, 27, 31, 0, 32, 1, 22, 5, 33, 10, 14, 3, 28, 20, 3, 35, 33, 38, 31, 12, 2, 18, 7, 31, 20, 9, 31], [17, 4, 28, 18, 4, 32, 0, 32, 5, 18, 12, 15, 7, 13, 37, 33, 38, 23, 38, 32, 10, 33, 29, 9], [3, 33, 20, 36, 27, 19, 4, 31, 18, 8, 0, 32, 24, 19, 38, 23], [18, 8, 17, 12, 2, 8, 31, 7, 13, 5, 32, 8, 29, 26, 28, 9, 23], [23, 1, 30, 7, 29, 5, 25, 20, 1, 23, 7, 31, 10, 23, 7, 23], [25, 6, 32, 5, 27, 36, 27, 5, 29, 33, 22, 34, 5, 29, 23, 27, 9, 29, 33, 20, 5, 13, 7, 29, 27, 7, 23, 4, 32], [5, 28, 6, 13, 28, 6, 13, 17, 8, 23, 9, 29], [20, 7, 37, 28, 7, 30, 31, 25, 4, 28, 33, 24, 28, 38, 23, 33, 5, 24, 4, 18, 12], [23, 7, 23, 4, 29, 1, 17, 5, 29, 31, 1, 27, 10, 26, 3, 22], [18, 7, 31, 7, 37, 5, 31, 28, 1, 30, 7, 13, 26, 3, 30, 31, 5, 28], [20, 1, 7, 24, 29, 6, 32, 37, 24, 10, 23, 22, 16, 26, 25, 3, 26, 33, 31], [7, 25, 19, 38, 23, 38, 24, 36, 33, 38, 7, 33], [27, 0, 32, 26, 31, 27, 5, 29, 14, 7, 30, 7, 24, 37, 3, 27, 30, 5, 28], [18, 7, 31, 26, 7, 23, 37, 25, 32, 36, 37, 22, 3, 23], [18, 5, 33, 32, 10, 5, 28, 22, 5, 28, 38, 29, 37, 0, 32, 5, 25, 28, 36, 33], [18, 5, 14, 3, 23, 36, 35, 3, 29, 7, 14, 33], [22, 28, 0, 26, 8, 23, 7, 37, 17, 5, 29, 3, 29, 31, 12, 6, 25, 12, 23, 22, 10, 4, 26, 31, 30, 12, 33, 31], [18, 8, 17, 12, 14, 3, 33, 12, 23], [18, 8, 30, 0, 28, 7, 14, 33, 18, 5, 17, 7, 29, 23, 14, 1, 28, 23], [7, 33, 17, 0, 37, 33, 10, 27, 33, 38, 24, 36, 5, 30, 27, 10, 31, 4, 28, 25], [17, 10, 0, 32, 19, 38, 33, 32, 10, 7, 13, 33, 38, 17, 12, 1, 27, 1], [18, 5, 15, 12, 23, 26, 32, 6, 28, 7, 13, 27, 3, 29, 25, 6, 32, 31, 33, 20, 7, 27, 31, 4, 28, 25, 7, 32, 4, 26, 33], [14, 1, 31, 1, 27, 23, 7, 32, 5, 33, 8, 33, 5, 23], [29, 36, 22, 0, 23, 1, 28, 10, 26, 31, 31, 29, 8, 26, 31], [31, 36, 17, 16, 23, 32, 8, 23, 0, 32, 30, 7, 26, 7, 33, 14, 7, 30, 31], [18, 8, 31, 12, 35, 26, 32, 3, 26, 33, 17, 1, 33, 36, 33, 31, 6, 32, 26, 6, 32, 29, 27, 1, 28], [31, 1, 19, 38, 7, 29, 5, 22, 9, 33, 3, 29, 9, 12], [20, 1, 22, 7, 24, 3, 29, 33, 38, 30, 3, 26, 18, 5, 31, 3, 23, 5, 28, 22, 3, 24, 37], [20, 1, 2, 4, 26, 33, 7, 29, 33, 38, 5, 27, 36, 33, 4, 28, 5, 29, 23, 23, 32, 36, 35, 23, 9, 29, 33, 9, 29], [32, 7, 37, 7, 31, 33, 5, 29, 31, 15, 12, 27, 0, 27, 5, 33, 12, 37], [31, 5, 23, 5, 29, 28, 1, 27, 10, 32, 1, 25, 28, 4, 26, 31, 5, 37, 0, 32, 24, 6, 29], [18, 5, 24, 32, 8, 33, 22, 28, 3, 26, 28, 4, 30, 12, 23, 37], [18, 36, 37, 28, 5, 35, 28, 1, 17, 12, 28, 37, 20, 1, 2, 6, 32, 33, 5, 28, 23], [5, 25, 38, 28, 7, 14, 15, 6, 33, 26, 8, 27, 7, 29, 33, 38, 20, 7, 37, 20, 4, 23], [31, 29, 8, 26, 31, 0, 32, 5, 24, 28, 1, 20, 1, 31, 4, 23, 5, 24, 4, 29], [20, 1, 27, 34, 31, 5, 29, 23, 20, 7, 37, 28, 7, 30, 31, 5, 29, 1, 37, 5, 28, 1], [33, 38, 0, 29, 12, 20, 7, 27, 7, 37, 33, 38, 0, 29, 12, 9, 12, 31, 4, 28, 35, 37], [6, 28, 15, 32, 1, 27, 5, 31, 33, 26, 5, 27, 30, 4, 32, 29, 36, 33, 31, 5, 29, 23, 5, 24, 32, 1, 33, 38, 24, 36], [14, 1, 17, 4, 29, 33, 0, 29, 23, 7, 31, 32, 7, 24, 0, 32, 23, 7, 13, 27, 10, 30, 32, 36, 33, 4, 31, 33, 31], [3, 37, 19, 38, 26, 3, 29, 26, 9, 29, 33, 0, 29, 27, 1, 33, 38, 23, 38, 18, 5, 31, 8, 27], [3, 23, 15, 7, 13, 37, 3, 37, 19, 38, 25, 10, 29, 23, 19, 38, 29, 1, 23, 4, 27], [19, 38, 26, 8, 27, 17, 4, 28, 7, 26, 17, 7, 30, 33, 33, 38, 23, 10], [5, 28, 0, 32, 21, 30, 1, 31, 5, 35, 4, 29, 21, 5, 29, 26, 9, 28, 7, 13, 35, 3, 29, 7, 14, 33], [20, 1, 29, 1, 23, 37, 9, 33, 25, 1, 28, 23, 12, 37, 22, 3, 23], [17, 4, 28, 18, 4, 29, 20, 38, 22, 32, 6, 33, 7, 33], [18, 5, 28, 10, 25, 22, 36, 33, 31, 17, 12, 31, 33, 5, 26, 25, 3, 31, 33], [17, 4, 28, 33, 6, 26, 36, 35, 12, 3, 33, 19, 6, 32, 6, 25, 5, 31], [18, 8, 25, 10, 29, 23, 23, 1, 30, 30, 4, 31, 5, 27, 7, 37, 5, 27, 7, 29, 18, 4, 27], [17, 10, 29, 0, 33, 33, 32, 10, 5, 29, 5, 18, 12, 26, 28, 5, 22], [18, 5, 17, 12, 26, 31, 0, 32, 30, 32, 1, 37, 4, 29, 33, 5, 23, 26, 32, 0, 29, 5, 28, 0, 21, 7, 26, 28, 1], [20, 7, 37, 27, 3, 29, 20, 16, 23, 20, 3, 23, 22, 7, 29, 5, 33, 3, 26, 33], [20, 9, 31, 8, 25, 7, 37, 33, 38, 31, 8, 25], [7, 37, 20, 7, 37, 3, 30, 5, 33, 10, 33, 7, 27, 30, 32, 38, 35, 23], [18, 5, 4, 32, 31, 27, 4, 28, 23, 17, 6, 32, 27, 7, 14, 5, 29, 23, 25, 9, 28], [1, 18, 12, 18, 3, 33, 6, 32, 5, 35, 4, 33, 32, 5, 29, 4, 32, 1, 5, 29], [20, 38, 17, 0, 29, 33, 31, 18, 7, 31, 23, 1, 23, 23, 5, 29], [7, 33, 20, 4, 28, 30, 31, 18, 36, 37, 30, 1, 30, 5, 28, 20, 38, 20, 4, 28, 30, 18, 4, 27, 31, 4, 28, 35, 37], [20, 1, 26, 16, 23, 33, 8, 26, 18, 5, 3, 23, 35, 10, 31, 6, 32, 28, 1, 35, 7, 33], [28, 4, 33, 5, 31, 29, 9, 24, 7, 35, 31, 5, 27, 15, 6, 33, 33, 38, 18, 5, 31, 36, 28], [28, 10, 26, 20, 7, 37, 24, 28, 6, 31, 1, 22, 28, 3, 26, 20, 4, 32], [19, 38, 25, 1, 28, 20, 7, 27, 4, 35, 12, 1, 27, 10, 28, 25, 12, 18, 12, 5, 17, 8], [31, 4, 33, 5, 31, 10, 23, 33, 38, 23, 32, 10, 15, 12, 36, 28, 1], [21, 10, 32, 36, 26, 5, 27, 30, 5, 31, 20, 4, 23, 7, 13], [20, 1, 26, 7, 31, 33, 20, 12, 6, 28, 31, 36, 5, 29, 23, 17, 7, 18, 23, 1, 30, 33, 4, 29, 23, 12, 29, 5, 31], [33, 12, 29, 14, 8, 26, 12, 5, 30, 31, 10, 23, 23, 9, 29], [22, 8, 31, 22, 6, 28, 37, 29, 36, 31, 7, 29, 2], [23, 32, 5, 13, 26, 5, 29, 29, 5, 31, 17, 0, 37, 29, 36, 28, 6, 13, 24, 12, 33, 0, 28, 12, 8, 33, 5, 23], [20, 7, 37, 26, 0, 27, 32, 3, 23, 37, 17, 12, 6, 28, 23, 4, 23], [17, 4, 32, 23, 38, 19, 38, 30, 16, 33, 18, 5, 28, 10, 33, 12, 25, 28, 38, 5, 23, 20, 0, 20, 0], [27, 1, 29, 17, 10, 28, 31, 30, 32, 7, 13, 20, 3, 23, 30, 3, 31, 33, 17, 4, 28, 7, 29, 33, 38, 31, 5, 27, 12], [29, 4, 26, 31, 33, 20, 1, 5, 23, 32, 4, 31, 33, 20, 7, 27, 31, 4, 28, 25, 33, 38, 9, 12, 31, 36, 28, 37], [5, 35, 26, 6, 32, 31, 23, 7, 32, 14, 1, 31, 4, 23], [14, 3, 28, 17, 1, 33, 1, 2, 20, 7, 27, 31, 5, 27], [20, 9, 23, 38, 17, 1, 23, 7, 25, 10, 29, 7, 33], [17, 4, 28, 29, 9, 17, 1, 20, 3, 35, 33, 38, 22, 7, 24, 15, 1, 5, 33, 12, 37], [33, 17, 10, 31, 20, 1, 29, 7, 32, 28, 1, 25, 4, 28], [10, 23, 1, 5, 37, 0, 32, 18, 5, 15, 32, 38, 17, 8, 33, 38, 29, 36, 17, 4, 32], [32, 5, 25, 26, 28, 6, 15, 32, 3, 31, 30, 33, 20, 7, 37, 25, 7, 31, 33], [22, 7, 20, 10, 29, 23, 20, 7, 27, 20, 7, 37, 27, 8, 33, 31, 31, 33, 12, 23, 17, 7, 15, 9, 33, 17, 8, 26, 7, 13], [31, 5, 27, 17, 5, 29, 32, 1, 37, 5, 29, 5, 22, 28, 1, 33, 32, 5, 31, 33, 17, 12, 18, 1], [26, 3, 29, 33, 31, 1, 27, 33, 38, 28, 36, 26, 8, 33, 28, 3, 29, 23, 27, 0, 32, 26, 31, 7, 29, 18, 7, 31, 31, 29, 36], [17, 6, 32, 29, 15, 32, 38, 3, 33, 18, 5, 29, 1, 37], [6, 28, 19, 6, 32, 17, 7, 14, 25, 5, 28, 15, 7, 13, 26, 7, 13, 17, 36, 29, 33, 2, 8, 29, 21, 18, 3, 33], [19, 4, 31, 27, 5, 27, 7, 33, 14, 16, 32, 17, 0, 37], [29, 36, 30, 36, 5, 33, 32, 1, 29, 36, 4, 32, 30, 28, 8, 29, 37, 29, 36, 23, 3, 29, 31, 12, 37], [26, 7, 29, 23, 5, 28, 10, 26, 5, 37, 0, 27, 22, 1], [22, 36, 15, 26, 0, 32, 37, 17, 12, 31, 28, 10, 33, 28, 1, 23, 3, 27, 5, 21, 23], [29, 36, 33, 4, 27, 33, 8, 14, 5, 29, 29, 36, 35, 12, 2, 38], [22, 32, 8, 26, 31, 14, 32, 1, 26, 33, 22, 7, 20, 10, 29, 23, 5, 31], [22, 5, 33, 4, 35, 32, 1, 22, 0, 23, 1, 26, 6, 28, 37, 19, 38, 21, 38, 29, 19, 12], [29, 6, 32, 7, 37, 14, 1, 5, 17, 4, 33, 22, 36, 33], [30, 9, 23, 12, 25, 28, 8, 27, 24, 5, 14, 33, 22, 7, 29, 1, 15, 18, 5, 17, 3, 24, 5, 29], [17, 1, 20, 3, 35, 29, 5, 15, 7, 13, 33, 38, 20, 10, 23, 5, 29, 23, 12, 5, 22, 16, 14, 5, 28], [15, 3, 13, 26, 19, 38, 14, 1, 31, 4, 23, 23, 5, 31, 33, 7, 13, 20, 12, 31, 4, 28, 25, 6, 25], [20, 1, 28, 16, 26, 33, 28, 8, 37, 1, 31, 30, 34, 28, 23, 5, 28, 7, 33, 5, 28, 26, 17, 4, 32, 5, 28, 5, 31], [17, 16, 23, 31, 5, 2, 3, 29, 3, 26, 33, 5, 35, 32, 5, 25, 19, 38, 37, 5, 28, 22, 1, 19, 38, 31, 25, 5, 28], [17, 5, 33, 23, 7, 23, 19, 38, 27, 1, 29, 22, 10, 18, 3, 33, 32, 3, 33, 5, 28, 31, 29, 8, 26, 24, 3, 24], [31, 33, 36, 29, 17, 4, 32, 26, 28, 8, 25, 6, 32, 33, 10, 28, 37], [20, 7, 37, 35, 34, 31, 31, 1, 27, 23, 15, 7, 26, 5, 29, 23, 30, 12, 30, 5, 31, 28, 5, 31], [30, 1, 30, 5, 28, 29, 4, 35, 12, 28, 10, 35, 25, 12, 4, 35, 12], [18, 3, 33, 7, 37, 17, 5, 33, 2, 10, 28, 23, 20, 16, 23, 7, 37, 20, 1, 33, 36, 28, 23, 20, 7, 27, 31, 4, 28, 25], [20, 1, 33, 31, 22, 3, 23, 25, 6, 32, 25, 32, 6, 31, 33, 22, 10, 33], [17, 7, 13, 27, 5, 29, 31, 33, 8, 26, 28, 7, 32, 20, 1, 30, 32, 8, 23], [18, 8, 1, 18, 12, 17, 6, 26, 33, 6, 32, 17, 12, 23, 32, 7, 35, 5, 29], [20, 1, 25, 6, 33, 18, 5, 30, 3, 29, 7, 26, 5, 35, 35, 12, 33, 7, 24, 36], [18, 5, 24, 0, 28, 25, 7, 13, 25, 0, 18, 12, 37, 32, 38, 28, 23, 7, 29, 20, 7, 37, 25, 8, 35, 12], [31, 5, 27, 15, 7, 13, 30, 16, 28, 23, 27, 10, 28, 4, 24], [17, 7, 28, 19, 38, 33, 4, 28, 27, 1, 17, 10], [27, 4, 29, 1, 14, 8, 30, 31, 7, 29, 22, 3, 15, 33, 5, 22, 37], [27, 12, 31, 5, 29, 4, 32, 1, 33, 12, 27, 5, 35, 0, 29, 12], [31, 36, 26, 5, 30, 28, 36, 26, 5, 28, 26, 5, 28, 12], [18, 4, 29, 17, 1, 17, 12, 6, 25, 5, 24, 4, 29], [22, 5, 33, 18, 4, 29, 32, 4, 23, 20, 4, 23, 37, 0, 32, 6, 25, 5, 29, 30, 8, 28], [29, 36, 22, 0, 23, 1, 23, 5, 37, 22, 5, 33, 18, 8, 24, 8, 35, 27, 1, 33, 4, 29, 28, 3, 31, 33, 19, 7, 32], [18, 5, 26, 7, 23, 20, 3, 37, 29, 36, 27, 3, 29, 12, 37, 22, 34, 37], [28, 1, 35, 27, 1, 19, 6, 32, 3, 23, 32, 4, 31], [18, 4, 32, 7, 37, 29, 36, 23, 7, 31, 30, 19, 38, 33, 7, 29, 35, 0, 28, 35, 7, 13, 25, 1, 28, 23, 17, 12, 26], [20, 1, 33, 32, 3, 35, 5, 28, 23, 6, 28, 36, 35, 12, 18, 5, 17, 12, 28, 23], [5, 29, 23, 31, 36, 20, 1, 17, 6, 26, 33, 8, 27, 28, 5, 31, 5, 24, 4, 29], [20, 1, 33, 16, 26, 5, 22, 7, 24, 31, 17, 7, 24, 5, 35, 20, 7, 37, 23, 32, 7, 13, 26], [33, 4, 29, 5, 18, 12, 37, 27, 8, 23, 29, 36, 32, 7, 30, 28, 10], [20, 1, 17, 6, 32, 5, 22, 32, 9, 29, 29, 7, 33, 31, 30, 6, 32, 33, 14, 12, 33, 17, 7, 18, 29, 36, 33, 10], [2, 4, 26, 18, 5, 36, 35, 12, 29, 10, 33, 12, 37, 9, 33], [0, 32, 19, 38, 5, 15, 32, 36, 12, 5, 35, 25, 28, 8, 27, 27, 12, 1, 29], [31, 36, 32, 38, 28, 37, 17, 1, 27, 8, 23, 7, 29, 5, 29, 5, 22, 3, 14, 33, 26, 5, 28, 38, 11, 5, 29], [23, 5, 37, 18, 7, 31, 27, 8, 26, 7, 33, 4, 29, 1, 1, 37, 1, 12, 26, 9, 12, 23], [17, 5, 29, 17, 10, 33, 20, 3, 29, 23, 30, 34, 37, 23, 5, 31, 33, 7, 26, 5, 22, 5, 35, 20, 7, 37, 23, 4, 31, 26], [5, 29, 5, 18, 12, 31, 29, 0, 32, 5, 28, 23, 26, 28, 36, 31, 36, 35, 12, 20, 4, 23], [36, 29, 36, 29, 0, 33, 5, 24, 4, 29, 20, 1, 31, 4, 23, 5, 28, 9, 23], [20, 36, 30, 33, 38, 31, 1, 19, 38, 5, 24, 4, 29], [18, 4, 32, 7, 37, 29, 36, 31, 33, 6, 32, 20, 1, 30, 10, 30, 33, 33, 32, 4, 27, 19, 5, 28, 5, 31, 28, 1], [17, 10, 23, 38, 17, 1, 29, 1, 23, 22, 7, 24, 12, 5, 29, 23, 22, 4, 33, 12, 22, 0, 27, 37], [21, 38, 29, 19, 12, 17, 5, 33, 0, 29, 12, 15, 31, 18, 5, 27, 3, 33, 12, 17, 7, 18, 19, 38], [31, 36, 20, 1, 5, 29, 23, 12, 31, 33, 16, 23, 20, 12, 30, 3, 29, 7, 26], [22, 5, 33, 32, 38, 28, 4, 33, 31, 29, 0, 33, 27, 10, 24, 8, 27], [17, 4, 29, 14, 4, 23, 18, 8, 28, 1, 35, 32, 5, 31, 33, 27, 0, 32, 26, 31], [20, 5, 28, 36, 4, 29, 1, 17, 5, 29, 3, 33, 20, 36, 27], [7, 33, 7, 37, 18, 5, 31, 8, 27, 36, 28, 31, 8, 27, 33, 4, 28, 27, 1, 7, 33, 31, 29, 8, 27], [22, 7, 33, 12, 5, 29, 32, 1, 37, 29, 7, 13, 21, 4, 28, 5, 31, 1], [32, 5, 31, 1, 35, 7, 13, 29, 36, 3, 29, 31, 12, 18, 8, 31, 4, 33, 18, 5, 25, 10, 12], [18, 8, 17, 12, 30, 12, 31, 38, 7, 13, 20, 7, 27], [18, 4, 29, 18, 5, 33, 4, 28, 5, 25, 36, 29, 7, 13, 22, 7, 24, 3, 29], [18, 5, 17, 3, 24, 5, 29, 37, 17, 12, 22, 12, 29, 7, 13, 25, 7, 32, 31, 28, 1], [14, 7, 35, 12, 7, 13, 20, 1, 30, 16, 33, 0, 29, 20, 7, 37, 26, 28, 36, 18, 37], [17, 4, 29, 14, 1, 5, 17, 36, 26, 14, 1, 17, 0, 37, 18, 5, 14, 7, 30], [25, 32, 5, 27, 2, 10, 28, 23, 20, 16, 23, 20, 1, 20, 3, 23, 29, 36, 29, 6, 28, 5, 22, 9, 33, 29, 10, 35, 37], [14, 1, 31, 29, 0, 32, 5, 28, 23, 17, 6, 32, 29, 7, 13, 28, 1], [29, 10, 31, 1, 35, 7, 29, 7, 25, 5, 33, 32, 10, 25, 5, 28, 24, 6, 23, 1], [3, 22, 31, 5, 28, 38, 14, 5, 29, 25, 6, 32, 20, 7, 37, 28, 10], [20, 12, 20, 5, 27, 22, 7, 26, 8, 27, 5, 24, 12, 24, 5, 28, 5, 35, 31, 12, 30, 32, 10, 37], [20, 9, 17, 7, 28, 17, 1, 17, 12, 26, 7, 33, 9, 33], [30, 16, 14, 22, 3, 26, 5, 30, 5, 29, 23, 32, 7, 30, 1, 33], [5, 32, 3, 22, 5, 33, 30, 5, 29, 2, 32, 1, 23, 5, 22, 5, 28, 23, 20, 7, 27], [20, 1, 32, 36, 23, 28, 36, 0, 29, 18, 5, 27, 4, 32, 37, 29, 4, 26], [18, 8, 29, 1, 18, 12, 24, 8, 29, 23, 29, 6, 32, 25, 4, 28, 22, 3, 26], [18, 5, 26, 28, 9, 23, 22, 12, 31, 33, 26, 5, 33, 6, 25, 5, 22, 32, 5, 30, 33, 28, 1], [20, 3, 35, 19, 38, 24, 0, 33, 7, 29, 5, 25, 22, 28, 3, 13, 26, 5, 33, 31], [18, 4, 32, 17, 0, 37, 27, 5, 2, 4, 27, 22, 32, 8, 31, 7, 13, 27, 5, 2, 7, 26, 31, 26, 28, 8, 27, 7, 13], [20, 7, 37, 20, 4, 23, 25, 28, 0, 30, 33, 22, 3, 26], [5, 29, 23, 19, 38, 15, 7, 13, 26, 19, 38, 20, 3, 35, 28, 3, 13, 24, 17, 5, 21, 30, 32, 0, 22, 28, 5, 27, 37], [3, 33, 28, 1, 31, 33, 18, 5, 17, 1, 28, 37, 23, 5, 24, 7, 29], [17, 10, 29, 6, 32, 5, 18, 5, 35, 4, 32, 1, 10, 23, 1, 5], [14, 1, 28, 16, 26, 33, 3, 33, 27, 1, 30, 32, 36, 35, 0, 26, 5, 33, 7, 35, 28, 1], [1, 18, 12, 22, 4, 33, 12, 29, 6, 32, 17, 12, 31], [24, 36, 2, 8, 29, 21, 19, 6, 32, 14, 38, 37, 22, 7, 25, 6, 32, 19, 38, 33, 12, 29, 12, 9, 29, 23], [2, 4, 31, 29, 5, 33, 31, 0, 32, 31, 33, 0, 32, 2, 1], [7, 33, 6, 28, 33, 8, 26, 31, 30, 28, 8, 31, 7, 29, 18, 5, 8, 33, 1, 29, 15, 31, 4, 29, 2, 12, 1], [7, 33, 27, 10, 33, 20, 12, 33, 19, 38, 18, 36], [22, 36, 15, 25, 7, 24, 19, 12, 37, 17, 16, 23, 24, 36, 20, 10, 12, 7, 29, 28, 8, 33, 12, 19, 7, 32, 37], [18, 5, 20, 36, 33, 4, 28, 36, 29, 12, 14, 32, 5, 24, 23], [18, 3, 33, 33, 5, 2, 33, 6, 25, 5, 33, 36, 33, 5, 28, 31, 33, 3, 27, 30, 1, 23], [20, 8, 26, 5, 27, 22, 3, 26, 20, 1, 14, 9, 33, 5, 23], [18, 5, 14, 4, 32, 5, 25, 31, 31, 17, 7, 35, 5, 28, 2, 4, 32, 33, 7, 28, 33, 5, 23, 22, 3, 26], [22, 4, 35, 32, 7, 21, 7, 37, 0, 32, 27, 8, 23, 25, 32, 5, 27, 31, 1, 23, 37, 18, 5, 17, 12, 28, 23, 36, 35, 12], [27, 0, 28, 5, 31, 26, 31, 0, 32, 5, 26, 8, 31, 7, 29, 30, 34, 29, 33], [18, 8, 17, 12, 33, 4, 18, 12, 23, 0, 32, 27, 1, 31, 33, 10, 28, 0, 29, 31, 33, 8, 22, 5, 28, 28, 10, 29, 37], [18, 5, 33, 8, 30, 31, 14, 36, 18, 3, 33], [27, 8, 22, 1, 31, 5, 27, 5, 18, 12, 23, 8], [17, 7, 18, 1, 2, 31, 6, 13, 20, 1, 24, 8, 35, 35, 12, 22, 5, 28, 25, 16, 33, 29, 36, 33, 31], [18, 4, 29, 17, 4, 23, 32, 7, 28, 1, 20, 3, 35, 31, 5, 27, 30, 28, 8, 31, 33, 38, 24, 36], [32, 1, 28, 22, 32, 10, 33, 5, 28, 6, 13, 18, 36, 37, 28, 10, 29, 37, 19, 38, 27, 10, 33, 31, 8], [5, 29, 23, 29, 8, 15, 5, 29, 23, 7, 23, 20, 1, 31, 1, 18, 4, 27, 33, 38], [18, 7, 31, 26, 36, 33, 28, 16, 26, 31, 28, 10, 26, 5, 32, 3, 24, 20, 1, 30], [17, 5, 29, 1, 35, 7, 29, 24, 8, 35, 27, 10, 28, 7, 33, 5, 28, 23, 6, 24, 5, 22, 7, 31, 26, 5, 33], [29, 0, 33, 17, 4, 29, 14, 4, 23, 17, 8, 33, 5, 23, 31, 36, 28, 6, 13, 6, 28, 32, 4, 23, 1], [7, 33, 31, 29, 4, 35, 12, 32, 6, 13, 7, 25, 28, 5, 35, 7, 37, 32, 1, 28], [22, 5, 33, 29, 9, 14, 1, 28, 16, 26, 33, 5, 24, 28, 1], [5, 30, 38, 28, 7, 37, 29, 36, 30, 28, 8, 31, 25, 6, 32, 5, 14, 5, 33, 33, 32, 3, 30], [31, 5, 27, 17, 7, 27, 5, 29, 24, 4, 33, 5, 32, 1, 28, 15, 32, 7, 28, 9, 33, 5, 35, 20, 9, 31, 17, 12, 26], [19, 38, 28, 5, 26, 1, 26, 7, 23, 37, 20, 1, 31, 4, 23], [7, 33, 24, 8, 35, 20, 12, 5, 28, 38, 23, 17, 7, 13, 26, 7, 13, 7, 25, 4, 26, 33], [29, 1, 18, 12, 3, 26, 29, 0, 28, 7, 21, 23, 18, 5, 24, 7, 25, 33], [31, 5, 26, 31, 4, 31, 25, 6, 32, 27, 4, 29, 1, 33, 12, 29, 30, 10, 26, 31, 20, 3, 37, 26, 5, 27, 20, 0, 32, 23], [17, 0, 2, 7, 33, 22, 7, 24, 14, 0, 33, 5, 20, 6, 32, 31, 35, 34, 31, 19, 4, 28, 23, 22, 3, 26], [18, 8, 17, 12, 6, 28, 32, 4, 23, 1, 31, 17, 36, 28, 5, 29, 33, 38, 22, 12, 31, 33, 7, 13], [23, 38, 19, 38, 29, 36, 17, 4, 32, 27, 10, 33, 20, 3, 35, 24, 6, 29], [20, 9, 0, 29, 12, 15, 23, 38, 19, 38, 27, 3, 29, 5, 21, 7, 33], [14, 1, 36, 35, 12, 30, 28, 8, 23, 20, 12, 20, 3, 29, 23], [18, 8, 26, 16, 23, 17, 6, 26, 32, 10, 23, 0, 29, 5, 22, 5, 31, 6, 32, 22, 1, 23, 32, 7, 35, 5, 29], [27, 10, 23, 32, 4, 31, 29, 1, 23, 37, 31, 5, 27, 17, 12, 26, 0, 29, 7, 33], [18, 5, 27, 5, 23, 17, 3, 24, 5, 29, 20, 3, 23, 26, 0, 33, 25, 10, 12, 6, 28, 31, 36], [20, 38, 7, 37, 24, 36, 7, 13, 33, 38, 31, 33, 0, 30, 27, 1], [24, 0, 33, 18, 5, 5, 30, 31, 33, 4, 32, 37, 24, 10, 20, 1, 22, 4, 28, 36, 37], [18, 7, 31, 22, 12, 15, 28, 4, 13, 26, 15, 31, 1, 27, 37, 33, 38, 22, 1, 33, 7, 30, 5, 26, 5, 28], [1, 2, 5, 35, 5, 31, 20, 3, 23, 20, 7, 37, 36, 29, 31, 30, 4, 14, 5, 28, 33, 1, 22, 7, 31, 10, 23, 37], [19, 38, 27, 1, 29, 5, 24, 8, 27, 17, 7, 18, 26, 0, 32, 23, 37], [20, 5, 28, 36, 22, 0, 31, 20, 1, 31, 4, 23, 5, 29, 23, 24, 32, 7, 29, 23], [20, 1, 27, 4, 33, 29, 36, 22, 0, 23, 1, 20, 1, 29, 38, 0, 29, 18, 7, 31, 17, 6, 26], [17, 10, 31, 4, 23, 18, 5, 33, 1, 2, 12, 0, 32, 33, 18, 9, 31, 36, 28, 8, 33], [29, 9, 18, 5, 20, 36, 30, 17, 0, 37, 24, 6, 29], [28, 10, 26, 3, 37, 7, 25, 7, 33, 17, 12, 22, 7, 28, 33, 5, 35, 22, 16, 26, 31], [27, 8, 22, 1, 18, 8, 17, 7, 28, 33, 8, 26, 5, 31], [23, 5, 37, 18, 7, 31, 22, 0, 18, 12, 19, 38], [18, 7, 31, 33, 38, 28, 26, 3, 29, 6, 28, 31, 36, 22, 1, 27, 8, 23, 17, 7, 18, 5, 28, 8, 18], [17, 12, 29, 0, 33, 23, 32, 5, 13, 26, 12, 23, 37, 14, 1, 31, 4, 23], [33, 38, 20, 7, 37, 31, 12, 30, 32, 10, 37, 20, 7, 37, 30, 28, 3, 29, 17, 12, 26, 33, 30, 12, 25, 5, 26, 33, 28, 1], [18, 5, 22, 38, 31, 33, 7, 37, 20, 4, 28, 30, 25, 5, 28, 22, 5, 33, 7, 29, 3, 23, 5, 26, 17, 5, 33], [36, 14, 5, 26, 31, 18, 5, 24, 12, 28, 31, 4, 23], [25, 10, 28, 33, 38, 5, 31, 27, 38, 18, 25, 7, 29, 7, 14], [18, 5, 26, 0, 32, 23, 6, 32, 26, 32, 3, 14, 33, 14, 5, 33], [18, 4, 32, 17, 0, 37, 6, 28, 31, 36, 5, 23, 6, 24, 5, 23, 7, 13, 24, 36, 23, 6, 24], [30, 12, 25, 4, 26, 33, 20, 1, 15, 6, 33], [7, 29, 33, 4, 28, 5, 21, 5, 29, 31, 21, 3, 22, 23, 3, 33, 20, 7, 27, 5, 26, 19, 38, 37, 7, 13, 28, 1], [36, 27, 5, 18, 12, 10, 31, 6, 18, 4, 27], [14, 1, 26, 3, 29, 23, 7, 26, 32, 1, 31, 18, 5, 29, 5, 27, 22, 12, 5, 35, 33, 4, 27, 33, 8, 14, 5, 29, 37], [31, 38, 29, 31, 4, 23, 18, 5, 20, 3, 25, 27, 5, 29], [29, 36, 22, 0, 23, 1, 4, 28, 31, 23, 7, 23, 1, 18, 12], [5, 18, 12, 37, 17, 12, 7, 25, 4, 27, 12, 5, 28], [22, 5, 33, 17, 5, 33, 7, 25, 31, 5, 27, 22, 0, 23, 1, 23, 7, 31, 10, 23, 37, 33, 38, 22, 32, 8, 26, 7, 33], [29, 6, 32, 5, 10, 15, 7, 13, 26, 19, 38, 7, 27, 3, 21, 5, 29, 7, 33], [18, 4, 29, 26, 8, 27, 26, 36, 26, 5, 29, 5, 33, 31, 4, 24, 37, 5, 29, 23, 32, 10, 31, 17, 10, 29], [25, 10, 33, 31, 31, 30, 32, 3, 13, 5, 30, 5, 29, 23, 17, 12, 26, 17, 7, 26, 28, 1, 31, 26, 17, 4, 28, 2, 33], [4, 35, 32, 1, 15, 7, 13, 17, 4, 29, 33, 32, 1, 28, 31, 27, 38, 18, 18, 5, 14, 4, 32, 5, 25, 31, 4, 23], [17, 4, 32, 17, 12, 18, 8, 29, 9], [14, 1, 26, 3, 29, 32, 1, 27, 38, 35, 6, 28, 29, 7, 26, 29, 3, 26, 31, 17, 7, 18, 7, 29, 32, 1, 2], [22, 5, 33, 17, 10, 30, 8, 20, 12, 22, 7, 28, 37], [20, 1, 32, 4, 26, 5, 24, 29, 10, 37, 23, 20, 7, 37, 21, 3, 26, 5, 33, 5, 29, 23, 33, 32, 9, 37, 12, 37], [7, 33, 31, 30, 12, 20, 3, 30, 31, 5, 27, 10, 28, 25, 32, 5, 27, 20, 1, 32, 17, 4, 32, 17, 1, 31, 7, 33], [29, 5, 29, 14, 16, 23, 3, 31, 26, 28, 4, 31], [20, 1, 31, 29, 3, 2, 33, 3, 33, 7, 33, 31, 3, 35, 7, 21, 28, 1], [17, 1, 22, 36, 15, 20, 3, 23, 20, 3, 13, 36, 35, 12, 37], [5, 18, 12, 17, 10, 37, 18, 5, 9, 33, 28, 16, 26, 26, 16, 23, 22, 1, 23, 0, 32, 26, 7, 29, 23, 1, 23], [17, 5, 33, 4, 35, 12, 19, 38, 33, 4, 28, 20, 7, 27, 20, 1, 17, 7, 28, 23, 10, 35], [27, 8, 22, 1, 7, 33, 17, 16, 23, 17, 12, 26, 18, 5, 5, 18, 12, 17, 8], [29, 36, 32, 7, 28, 1, 32, 10, 33, 3, 37, 32, 8, 29], [18, 3, 33, 17, 12, 27, 5, 27, 12, 23, 12, 12], [17, 5, 33, 9, 33, 25, 7, 33, 23, 5, 37, 14, 1, 23, 32, 10, 35, 25, 6, 32], [18, 4, 29, 31, 7, 27, 12, 25, 7, 25, 33, 1, 29, 27, 7, 29, 5, 33, 31, 28, 6, 13, 24, 12], [20, 8, 31, 33, 5, 28, 1, 18, 5, 22, 34, 31, 17, 7, 2, 33, 0, 29, 5, 31, 1, 28, 7, 13, 28, 10, 33], [24, 0, 14, 18, 5, 20, 9, 31, 23, 5, 37, 28, 16, 26, 28, 10, 26, 5, 27, 6, 32, 24, 18, 36], [20, 9, 4, 35, 12, 5, 22, 34, 37, 28, 10, 35, 28, 1, 10, 37, 27, 10, 33, 32, 36, 35], [7, 33, 17, 0, 37, 0, 26, 17, 12, 23, 35, 4, 32, 1, 0, 26, 17, 12, 23], [25, 6, 32, 17, 5, 33, 23, 38, 18, 5, 19, 38, 33, 36, 30, 1, 5, 29, 37, 28, 8, 22, 12], [20, 1, 27, 10, 33, 31, 8, 6, 32, 23, 38, 31, 5, 27, 15, 7, 13, 25, 38, 28, 7, 14], [17, 0, 37, 7, 33, 5, 22, 12, 15, 23, 8, 22, 6, 28], [31, 36, 20, 1, 20, 10, 23, 37, 18, 5, 27, 8, 5, 29, 8, 37], [32, 8, 29, 21, 17, 0, 37, 5, 35, 10, 33, 5, 28, 23, 7, 33, 8, 28], [14, 1, 27, 5, 31, 33, 20, 3, 35, 30, 16, 33, 20, 7, 37, 26, 28, 36, 18, 37, 7, 29, 18, 5, 26, 28, 0, 37, 5, 33], [7, 27, 24, 36, 7, 13, 33, 38, 31, 12, 2, 18, 7, 31, 20, 9, 31], [18, 8, 19, 38, 37, 23, 30, 7, 13, 26, 33, 3, 29, 6, 32, 26, 32, 1, 27, 30, 9, 23, 12], [22, 4, 32, 1, 18, 36, 37, 19, 38, 29, 5, 25, 6, 32, 27, 37, 31, 36, 18, 8, 17, 36, 29, 33, 22, 1, 25, 9, 29, 23], [28, 5, 28, 5, 22, 10, 5, 29, 23, 24, 16, 23, 29, 10, 33, 20, 7, 37, 35, 34, 31, 14, 16, 26], [17, 0, 37, 7, 33, 6, 28, 35, 8, 29, 28, 8, 22, 12], [18, 5, 22, 28, 38, 32, 5, 24, 17, 0, 37, 31, 5, 31, 30, 7, 14, 5, 31, 28, 1, 22, 32, 10, 33, 5, 29, 23, 29, 38], [24, 12, 7, 28, 5, 37, 17, 12, 32, 8, 31, 7, 13, 33, 5, 17, 6, 32, 23, 20, 7, 27], [18, 5, 26, 28, 12, 26, 31, 10, 37, 25, 28, 7, 26, 12, 23]]

train_labels = np.array(map(lambda i: label_map[i], train_labels))
val_labels = np.array(map(lambda i: label_map[i], val_labels))

bigram_count_map = {}
for i in range(len(label_map)):
    for j in range(len(label_map[i])-1):
        key = tuple(label_map[i][j:j+2])
        bigram_count_map[key] = bigram_count_map.get(key, 0) + 1
trigram_count_map = {}
for i in range(len(label_map)):
    for j in range(len(label_map[i])-2):
        key = tuple(label_map[i][j:j+3])
        trigram_count_map[key] = trigram_count_map.get(key, 0) + 1
fourgram_count_map = {}
for i in range(len(label_map)):
    for j in range(len(label_map[i])-3):
        key = tuple(label_map[i][j:j+4])
        fourgram_count_map[key] = fourgram_count_map.get(key, 0) + 1
fivegram_count_map = {}
for i in range(len(label_map)):
    for j in range(len(label_map[i])-4):
        key = tuple(label_map[i][j:j+5])
        fivegram_count_map[key] = fivegram_count_map.get(key, 0) + 1
unigram_count_map = {unigram: count for (unigram, count) in\
                     zip(*np.unique(np.concatenate(label_map), return_counts=True))}
print unigram_count_map
print bigram_count_map
print len(bigram_count_map)
print trigram_count_map
print len(trigram_count_map)

print np.shape(train_sequences)
print np.shape(train_labels)
print np.shape(val_sequences)
print np.shape(val_labels)

num_classes = len(np.unique(reduce(lambda a,b: a+b, label_map))) + 1

num_channels = len(train_sequences[0][0])


learning_rate = 1e-4 #5e-4 #1e-3
dropout_rate = 0.4

num_channels = 8

inputs = tf.placeholder(tf.float32,[None, None, num_channels]) #[batch_size,timestep,features]
targets = tf.sparse_placeholder(tf.int32)
sequence_lengths = tf.placeholder(tf.int32, [None])
weights = tf.placeholder(tf.float32, [None])
training = tf.placeholder(tf.bool)
batch_size = tf.shape(inputs)[0]
max_timesteps = tf.shape(inputs)[1]

lstm_hidden_size = 256
#lstm1 = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size, use_peepholes=True)
lstm1 = tf.contrib.rnn.LayerNormBasicLSTMCell(lstm_hidden_size, dropout_keep_prob=1.0-dropout_rate)
dropout1 = tf.nn.rnn_cell.DropoutWrapper(lstm1, 1.0-dropout_rate)
#lstm1b = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size, use_peepholes=True)
lstm1b = tf.contrib.rnn.LayerNormBasicLSTMCell(lstm_hidden_size, dropout_keep_prob=1.0-dropout_rate)
dropout1b = tf.nn.rnn_cell.DropoutWrapper(lstm1b, 1.0-dropout_rate)
#lstm2 = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size, use_peepholes=True)
lstm2 = tf.contrib.rnn.LayerNormBasicLSTMCell(lstm_hidden_size, dropout_keep_prob=1.0-dropout_rate)
dropout2 = tf.nn.rnn_cell.DropoutWrapper(lstm2, 1.0-dropout_rate)
#lstm2b = tf.nn.rnn_cell.LSTMCell(lstm_hidden_size, use_peepholes=True)
lstm2b = tf.contrib.rnn.LayerNormBasicLSTMCell(lstm_hidden_size, dropout_keep_prob=1.0-dropout_rate)
dropout2b = tf.nn.rnn_cell.DropoutWrapper(lstm2b, 1.0-dropout_rate)

forward_stack = tf.nn.rnn_cell.MultiRNNCell([dropout2])
backward_stack = tf.nn.rnn_cell.MultiRNNCell([dropout2b])

outputs, states = tf.nn.bidirectional_dynamic_rnn(forward_stack, backward_stack, inputs, dtype=tf.float32)
outputs = tf.concat(outputs, 2)

reshaped = tf.reshape(outputs, [-1, 2 * lstm_hidden_size])

reshaped = tf.layers.dense(reshaped, 1024, activation=tf.nn.relu,
            kernel_initializer=tf.initializers.truncated_normal(stddev=np.sqrt(2.0/(2 * lstm_hidden_size * 1024))))
reshaped = tf.layers.dense(reshaped, 1024, activation=tf.nn.relu,
            kernel_initializer=tf.initializers.truncated_normal(stddev=np.sqrt(2.0/(1024 * 1024))))
logits = tf.layers.dense(reshaped, num_classes,
            kernel_initializer=tf.initializers.truncated_normal(stddev=np.sqrt(2.0/(1024 * num_classes))))


logits = tf.reshape(logits, [batch_size, -1, num_classes])
logits = tf.transpose(logits, [1, 0, 2])

loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=sequence_lengths, time_major=True)
loss = tf.reduce_mean(tf.multiply(loss, weights))

# L2 regularization
#l2 = 0.005 * sum([tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\
#                          if 'noreg' not in tf_var.name.lower() and 'bias' not in tf_var.name.lower()])
#loss += l2

optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)
#optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)

#decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, sequence_lengths)
ctc_output = tf.nn.ctc_beam_search_decoder(logits, sequence_lengths, top_paths=10)
decoded, log_prob = ctc_output[0], ctc_output[1]

error = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32), targets, normalize=True))

decoded_selected = tf.sparse_placeholder(tf.int32)
error_selected = tf.reduce_mean(tf.edit_distance(decoded_selected, targets, normalize=True))


num_epochs = 20000 #200
batch_size = 20 #20 #50

num_training_samples = len(train_sequences)
num_validation_samples = len(val_sequences)
num_training_batches = max(1, int(num_training_samples / batch_size))
num_validation_batches = max(1, int(num_validation_samples / batch_size))
start_time = None
last_time = None

# Table display
progress_bar_size = 20
max_batches = max(num_training_batches, num_validation_batches)
layout = [
    dict(name='Ep.', width=len(str(num_epochs)), align='center'),
    dict(name='Batch', width=2*len(str(max_batches))+1, align='center'),
#    dict(name='', width=0, align='center'),
    dict(name='Progress/Timestamp', width=progress_bar_size+2, align='center'),
    dict(name='ETA/Elapsed', width=7, suffix='s', align='center'),
    dict(name='', width=0, align='center'),
    dict(name='Train Loss', width=8, align='center'),
    dict(name='Train Err', width=7, align='center'),
    dict(name='', width=0, align='center'),
    dict(name='Val Loss', width=8, align='center'),
    dict(name='Val Err', width=7, align='center'),
    dict(name='', width=0, align='center'),
    dict(name='Min Val Err', width=7, align='center'),
]

training_losses = []
training_errors = []
validation_losses = []
validation_errors = []

since_training = 0
def update_table(epoch, batch, training_loss, training_error, min_validation_error,
                 validation_loss=None, validation_error=None, finished=False):
    global last_time
    global since_training
    num_batches = num_training_batches if validation_loss is None else num_validation_batches
    progress = int(math.ceil(progress_bar_size * float(batch) / num_batches))
#    progress_string = '[' + '#' * progress + ' ' * (progress_bar_size - progress) + ']'
    status = ' Training' if validation_loss is None else ' Validating'
    status = status[:max(0, progress_bar_size - progress)]
    progress_string = '[' + '#' * progress + status + ' ' * (progress_bar_size - progress - len(status)) + ']'
    now = time.time()
    start_elapsed = now - start_time
    if validation_loss is None:
        epoch_elapsed = now - last_time
        since_training = now
    else:
        epoch_elapsed = now - since_training
    batch_time_estimate = epoch_elapsed / batch if batch else 0.0
    eta_string = batch_time_estimate * (num_batches - batch) or '--'
    if finished:
        epoch_elapsed = now - last_time
        last_time = now
        progress_string = time.strftime("%I:%M:%S %p",time.localtime())+'; '+str(int(start_elapsed*10)/10.)+'s'
        eta_string = epoch_elapsed
        training_losses.append(training_loss)
        training_errors.append(training_error)
        validation_losses.append(validation_loss)
        validation_errors.append(validation_error)
    table.update(epoch + 1, str(batch + 1) + '/' + str(num_batches),
                 progress_string, eta_string, '',
                 training_loss or '--', training_error or '--', '',
                 validation_loss or '--', validation_error or '--', '',
                 min_validation_error if finished else '--')
#    if finished and (epoch + 1) % 10 == 0:
#        print
#        print 'training_losses =', training_losses
#        print
#        print 'training_errors =', training_errors
#        print
#        print 'validation_losses =', validation_losses
#        print
#        print 'validation_errors =', validation_errors

def sparsify(labels):
    indices = []
    values = []
    for n, seq in enumerate(labels):
        indices.extend(zip([n]*len(seq), range(len(seq))))
        values.extend(seq)
    indices = np.asarray(indices, dtype=np.int64)
    values = np.asarray(values, dtype=np.int32)
    shape = np.asarray([len(labels), max(map(len, labels))], dtype=np.int64)
    return indices, values, shape

saver = tf.train.Saver()
with tf.Session() as session:
    tf.global_variables_initializer().run()

    table = DynamicConsoleTable(layout)
    table.print_header()

    start_time = time.time()
    last_time = start_time

    min_validation_error = float('inf')
    errors = []
    error_selecteds = []
    for epoch in range(num_epochs):
        training_loss = 0.0
        training_error = 0.0
        permutation = np.random.permutation(num_training_samples)
        train_sequences = train_sequences[permutation]
        train_labels = train_labels[permutation]
        train_weights = train_weights[permutation]
        training_decoded = []
        training_log_probs = []
        for batch in range(num_training_batches):            
            indices = range(batch * batch_size, (batch + 1) * batch_size)
            if batch == num_training_batches - 1:
                indices = range(batch * batch_size, num_training_samples)
            batch_sequences = train_sequences[indices]
            batch_lengths = map(len, batch_sequences)
            batch_sequences = data.transform.pad_truncate(batch_sequences, max(batch_lengths), position=0.0)
            batch_labels = train_labels[indices]
            batch_weights = train_weights[indices]
            sparse_batch_labels = sparsify(batch_labels)

            update_table(epoch, batch, training_loss / (batch_size * max(1, batch)),
                         training_error / (batch_size * max(1, batch)), min_validation_error)

            training_feed = {inputs: batch_sequences, targets: sparse_batch_labels,
                             sequence_lengths: batch_lengths, weights: batch_weights, training: True}
            batch_loss, _, batch_decoded, batch_error, batch_log_prob = \
                                    session.run([loss, optimizer, decoded, error, log_prob], training_feed)

            training_loss += batch_loss * len(indices)
            training_error += batch_error * len(indices)
#            batch_decoded = tf.sparse_tensor_to_dense(batch_decoded[0], default_value=-1).eval()
            batch_decoded = map(lambda x: tf.sparse_tensor_to_dense(x, default_value=-1).eval(), batch_decoded)
            for i in range(len(batch_decoded[0])):
                training_decoded.append([])
                for j in range(len(batch_decoded)):
                    training_decoded[-1].append(batch_decoded[j][i])
            for log_probs in batch_log_prob:
                training_log_probs.append(log_probs)

        training_loss /= num_training_samples
        training_error /= num_training_samples

        validation_loss = 0.0
        validation_error = 0.0
        validation_error_selected = 0.0
        permutation = np.random.permutation(num_validation_samples)
        val_sequences = val_sequences[permutation]
        val_labels = val_labels[permutation]
        validation_decoded = []
        validation_log_probs = []
        for batch in range(num_validation_batches):         
            indices = range(batch * batch_size, (batch + 1) * batch_size)
            if batch == num_validation_batches - 1:
                indices = range(batch * batch_size, num_validation_samples)
            batch_sequences = val_sequences[indices]
            batch_lengths = map(len, batch_sequences)
            batch_sequences = data.transform.pad_truncate(batch_sequences, max(batch_lengths), position=0.0)
            batch_labels = val_labels[indices]
            batch_weights = np.ones(len(indices))
            sparse_batch_labels = sparsify(batch_labels)

            update_table(epoch, batch, training_loss, training_error, min_validation_error,
                         validation_loss / (batch_size * max(1, batch)),
                         validation_error / (batch_size * max(1, batch)))

            validation_feed = {inputs: batch_sequences, targets: sparse_batch_labels,
                               sequence_lengths: batch_lengths, weights: batch_weights, training: False}
            batch_loss, batch_error, batch_decoded, batch_log_prob = \
                                            session.run([loss, error, decoded, log_prob], validation_feed)
            validation_loss += batch_loss * len(indices)
            validation_error += batch_error * len(indices)

            batch_decoded = map(lambda x: tf.sparse_tensor_to_dense(x, default_value=-1).eval(), batch_decoded)


            # modifies probabilities
            for i in range(len(batch_decoded)):
                for j in range(len(batch_decoded[i])):
                    tmp = filter(lambda x: x > -1, batch_decoded[i][j])
#                    print tmp
#                    print batch_log_prob[j][i]
                    for k in range(len(tmp)-1):
                        key = tuple(tmp[k:k+2])
                        batch_log_prob[j][i] += np.log(1.0 if key in bigram_count_map else 0.5)
                    for k in range(len(tmp)-2):
                        key = tuple(tmp[k:k+3])
                        batch_log_prob[j][i] += np.log(1.0 if key in trigram_count_map else 0.5)
                    for k in range(len(tmp)-3):
                        key = tuple(tmp[k:k+4])
                        batch_log_prob[j][i] += np.log(1.0 if key in fourgram_count_map else 0.5)
                    for k in range(len(tmp)-4):
                        key = tuple(tmp[k:k+5])
                        batch_log_prob[j][i] += np.log(1.0 if key in fivegram_count_map else 0.5)

            batch_decoded_selected = batch_decoded[0]
            batch_decoded_selected = map(list, map(lambda x: x[np.where(x > -1)], batch_decoded_selected))

#            print
#            print batch_decoded_selected

            batch_decoded_selected = []
            for i in range(len(batch_decoded[0])):
                pairs = [(batch_log_prob[i][j], batch_decoded[j][i]) for j in range(len(batch_decoded))]
                pairs = sorted(pairs, key=lambda x: x[0])
#                while len(pairs) > 1 and not len(pairs[-1][1]):
#                    pairs.pop()
                pairs = pairs[::-1]
                selected = pairs[0][1][np.where(pairs[0][1] > -1)]
                batch_decoded_selected.append(list(selected))
#            max_length = max(map(len, batch_decoded_selected))
#            for i in range(len(batch_decoded_selected)):
#                while len(batch_decoded_selected[i]) < max_length:
#                    batch_decoded_selected[i].append(-1)

#            print batch_decoded_selected

            sparse_indices, value, shape = sparsify(batch_decoded_selected)
            if not len(sparse_indices):
                sparse_indices = np.zeros((0, 2))

            selected_feed = {decoded_selected: (sparse_indices, value, shape), targets: sparse_batch_labels}
            batch_error_selected = session.run(error_selected, selected_feed)
            validation_error_selected += batch_error_selected * len(indices)

#            print batch_error, batch_error_selected

            for i in range(len(batch_decoded[0])):
                validation_decoded.append([])
                for j in range(len(batch_decoded)):
                    validation_decoded[-1].append(batch_decoded[j][i])
            for log_probs in batch_log_prob:
                validation_log_probs.append(log_probs)
        validation_loss /= num_validation_samples
        validation_error /= num_validation_samples
        validation_error_selected /= num_validation_samples
        
        if validation_error_selected < min_validation_error:
            model_name = 'ctc_lstm_phoneme400_test_model.ckpt'
            save_path = saver.save(session, os.path.join(abs_path, model_name))
            print ' Model saved:', model_name,
        min_validation_error = min(validation_error_selected, min_validation_error)

        errors.append(validation_error)
        error_selecteds.append(validation_error_selected)

        print
        print validation_error, validation_error_selected

#        plt.figure(1)
#        plt.gcf().clear()
#        plt.plot(errors)
#        plt.plot(error_selecteds)
#        plt.pause(0.00001)

        update_table(epoch, batch, training_loss, training_error,
                     min_validation_error, validation_loss, validation_error, finished=True)
        print
        print 'Training:'
        for i in range(min(5, len(training_decoded))):
#            print train_labels[i], ' => ', training_decoded[i][np.where(training_decoded[i] > -1)]
            print '\t', ' '.join(words[train_labels[i]]), ' =>'
            pairs = zip(training_log_probs[i], range(len(training_decoded[i])))
            pairs = sorted(pairs)[::-1]
            for j in range(len(training_decoded[i])):
                print '\t\t', np.exp(training_log_probs[i][j]), '\t', \
                ' '.join(words[training_decoded[i][j][np.where(training_decoded[i][j] > -1)]]), \
                '\t\t\t', np.exp(pairs[j][0]), '\t', \
                ' '.join(words[training_decoded[i][pairs[j][1]][
                            np.where(training_decoded[i][pairs[j][1]] > -1)]])
        print 'Validation:'
        for i in range(min(5, len(validation_decoded))):
#            print val_labels[i], ' => ', validation_decoded[i][np.where(validation_decoded[i] > -1)]
            print '\t', ' '.join(words[val_labels[i]]), ' =>'
            pairs = zip(validation_log_probs[i], range(len(validation_decoded[i])))
            pairs = sorted(pairs)[::-1]
            for j in range(len(validation_decoded[i])):
                print '\t\t', np.exp(validation_log_probs[i][j]), '\t', \
                ' '.join(words[validation_decoded[i][j][np.where(validation_decoded[i][j] > -1)]]), \
                '\t\t\t', np.exp(pairs[j][0]), '\t', \
                ' '.join(words[validation_decoded[i][pairs[j][1]][
                            np.where(validation_decoded[i][pairs[j][1]] > -1)]])

        reprint_header = (epoch+1) % 10 == 0 and epoch < num_epochs - 1
        table.finalize(divider=not reprint_header)
        if reprint_header:
            table.print_header()
