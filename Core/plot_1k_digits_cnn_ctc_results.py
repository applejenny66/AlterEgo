import numpy as np
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
import data

training_losses = [8.989609973882539, 4.072917522168627, 3.554538796929752, 2.8056902994517405, 2.2077337168400586, 1.8897245125053754, 1.7202029049006942, 1.4767456171559352, 1.2633460806865318, 1.0553113915561851, 0.9511443999857684, 0.8005436495238659, 0.6716432618159874, 0.562492308663387, 0.48227175953341467, 0.43640996077481436, 0.37077827732157864, 0.29839239011403, 0.28221163017297884, 0.2521161598707336, 0.24276638264749564, 0.17336905557735294, 0.1637142778319471, 0.11781970835199543, 0.10465969841659459, 0.07985605508868211, 0.04666073111439842, 0.043632329630306346, 0.02929224467720666, 0.01895334563686762, 0.021629264152127934, 0.012834915106056952, 0.01060139467586684, 0.006897174710639162, 0.00612639515885925, 0.004832751145038535, 0.004442977859011663, 0.003285070312105947, 0.0027754232622196084, 0.002825247455583094, 0.0019792713927945086, 0.002147646967832949, 0.0024026120218503125, 0.0018732294438747702, 0.0016717485538714266, 0.0014393563828205848, 0.0011315432854995034, 0.0011510072702852388, 0.0009371192111585022, 0.0012001009906027343, 0.000866523122847129, 0.0008952692676079916, 0.0007911970267480154, 0.0008313081606163405, 0.0006771140467083337, 0.000778623220885136, 0.0006407830952453652, 0.0007022025799409188, 0.0007464183023461183, 0.0006288177717257949, 0.0006244610002100029, 0.0006499805402473297, 0.0005352389321546831, 0.00048138734274330775, 0.0003906234598937718, 0.0006197441576255693, 0.0004420394501709714, 0.0003943885735531331, 0.0004163254280665926, 0.0004209196874095748, 0.000388963994672035, 0.00044256998751056835, 0.0003654227675367877, 0.00032772800239910875, 0.00034077186574946384, 0.0003115971530087633, 0.0003433982707374407, 0.0003102622059587803, 0.0003566060540893304, 0.0002783416658929044, 0.0002917425202516218, 0.0002633693268114786, 0.00023823860177490252, 0.0002760087642849125, 0.0002421126699035537, 0.00027702643392984967, 0.0002628947346428641, 0.00025283034711701835, 0.00026197027153673346, 0.00021442655836128525, 0.0002464243830782757, 0.0002397020466114376, 0.0002291237535388848, 0.00023243757930365116, 0.00022094767741186338, 0.00021102152261852586, 0.0001936277652918696, 0.00019567123986324113, 0.00019389250942694595, 0.00019430318492346022]

training_errors = [0.6318082676993476, 0.45098040540233936, 0.4270152545053195, 0.2679738812392054, 0.17211329693498176, 0.17211329800630706, 0.18082789297586951, 0.13289760686213675, 0.10675381864208022, 0.10893246862623426, 0.08932462324893553, 0.06535948058164198, 0.05882352907088847, 0.047930283800644034, 0.0370370394799834, 0.03921568710235209, 0.03267973783164242, 0.030501090696239783, 0.02396514202206353, 0.02832244121122594, 0.021786492537049687, 0.01525054509854979, 0.015250545369424657, 0.008714597227867523, 0.00871459716090969, 0.004357298613933761, 0.0, 0.0, 0.0021786492902274223, 0.0, 0.0021786493237063386, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

validation_losses = [4.208780765533447, 3.78359317779541, 3.1250956058502197, 2.3733575344085693, 1.9407906532287598, 2.066398859024048, 1.6323471069335938, 1.3470672369003296, 1.3667566776275635, 1.0768781900405884, 0.9912214279174805, 0.7731025815010071, 0.9224793910980225, 0.7652484774589539, 0.6759684085845947, 0.7706234455108643, 0.6451375484466553, 0.5189756155014038, 0.6390944123268127, 0.6757743954658508, 0.5556643009185791, 0.5621797442436218, 0.5371591448783875, 0.4964989423751831, 0.5366332530975342, 0.5176107287406921, 0.5539922118186951, 0.541526734828949, 0.4725653827190399, 0.4978352189064026, 0.5449045896530151, 0.47873345017433167, 0.4974891245365143, 0.5412551760673523, 0.5290170311927795, 0.4854189455509186, 0.4776686728000641, 0.5154415369033813, 0.538833737373352, 0.5283476710319519, 0.5055246353149414, 0.5021086931228638, 0.5113639235496521, 0.5296593308448792, 0.5418404936790466, 0.5467365384101868, 0.5435106754302979, 0.5330386757850647, 0.5253831148147583, 0.5228474140167236, 0.5237448215484619, 0.5240786671638489, 0.5236605405807495, 0.525689423084259, 0.5337086915969849, 0.5324625372886658, 0.5283223390579224, 0.529935359954834, 0.5329030156135559, 0.5318165421485901, 0.5331470966339111, 0.5346503853797913, 0.532733142375946, 0.5313298106193542, 0.528631865978241, 0.5236319899559021, 0.520653486251831, 0.5231980681419373, 0.5287207365036011, 0.5353574156761169, 0.5373808741569519, 0.541979968547821, 0.5416791439056396, 0.540909469127655, 0.5398170351982117, 0.5421007871627808, 0.548252522945404, 0.5522114038467407, 0.5514088273048401, 0.5519073605537415, 0.5510637760162354, 0.5487818121910095, 0.5478624701499939, 0.5454232096672058, 0.5439157485961914, 0.5432232618331909, 0.5426300168037415, 0.544378399848938, 0.5474768280982971, 0.550396740436554, 0.5541693568229675, 0.5578575730323792, 0.5613901615142822, 0.56449294090271, 0.5666719079017639, 0.5665692090988159, 0.5648685693740845, 0.5652306079864502, 0.5653888583183289, 0.5628995895385742]

validation_errors = [0.444444477558136, 0.444444477558136, 0.3777777850627899, 0.17777778208255768, 0.1666666716337204, 0.23333333432674408, 0.18888889253139496, 0.12222222238779068, 0.18888889253139496, 0.10000000149011612, 0.07777778059244156, 0.07777778059244156, 0.10000000149011612, 0.08888889104127884, 0.055555559694767, 0.06666667014360428, 0.06666667014360428, 0.04444444552063942, 0.06666667014360428, 0.06666667014360428, 0.055555559694767, 0.06666667014360428, 0.055555559694767, 0.04444444552063942, 0.055555559694767, 0.055555559694767, 0.04444444552063942, 0.055555559694767, 0.04444444552063942, 0.04444444552063942, 0.04444444552063942, 0.03333333507180214, 0.02222222276031971, 0.04444444552063942, 0.04444444552063942, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.04444444552063942, 0.02222222276031971, 0.02222222276031971, 0.03333333507180214, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971, 0.02222222276031971]

#kernel = np.array([1.0] * 15)
#kernel /= np.sum(kernel)
#validation_errors = data.transform.correlate(np.array(validation_errors)[:, None], kernel)[:,0]

f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))
plt.subplots_adjust(left=0.05, right=0.95, wspace=0.12)
plt.suptitle('Silent Speech 1kHz ({0,1,2} permutations)')
ax1.plot(training_losses[1:], label='Training Loss')
ax1.plot(validation_losses[1:], label='Validation Loss')
ax2.plot(training_errors[1:], label='Training Error')
ax2.plot(validation_errors[1:], label='Validation Error')
ax1.set_title('Losses (min train=' + str(min(training_losses))[:6] + ', min val=' + str(min(validation_losses))[:6] + ')')
ax2.set_title('Error Rates (min train=' + str(min(training_errors))[:6] + ', min val=' + str(min(validation_errors))[:6] + ')')
ax1.set_xlabel('Epoch')
ax2.set_xlabel('Epoch')
ax2.set_ylim((0.0, 2.0))
#ax1.set_yscale('log')
ax1.legend()
ax2.legend()
plt.show()